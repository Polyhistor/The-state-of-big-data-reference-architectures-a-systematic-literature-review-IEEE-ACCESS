@article{ELAZZAOUI2022103304,
  title    = {Blockchain-based delegated Quantum Cloud architecture for medical big data security},
  journal  = {Journal of Network and Computer Applications},
  volume   = {198},
  pages    = {103304},
  year     = {2022},
  issn     = {1084-8045},
  doi      = {https://doi.org/10.1016/j.jnca.2021.103304},
  url      = {https://www.sciencedirect.com/science/article/pii/S1084804521002952},
  author   = {Abir {EL Azzaoui} and Pradip Kumar Sharma and Jong Hyuk Park},
  keywords = {Quantum cloud-as-a-service, Blockchain, Cloud computing, Security, Privacy, Smart healthcare},
  abstract = {Smart Healthcare systems compromise complex computations such as visualization of molecules, analysis of DNA, and therapy determination. These are considered to be complex problems that today's supercomputers are still facing. On the other hand, Quantum computing promises fast, efficient, and scalable computing resources that are sufficient to compute large and complex operations in exponential time. It is a fact that Quantum computing will adequately innovate the computation perspective. However, it is not a feasible solution yet as it is likely to be rare and highly expensive to produce. This paper presents a Quantum Cloud-as-a-service for an efficient, scalable, and secure solution for complex Smart Healthcare computations. Our novelty resides in the usage of Quantum Terminal Machines (QTM) and Blockchain technology to enhance the feasibility and security of the proposed architecture. Experimental results prove the feasibility of the architecture and the absolute security of the implemented Q-OTP encryption.}
}
@article{BILAL2016144,
  title    = {Big data architecture for construction waste analytics (CWA): A conceptual framework},
  journal  = {Journal of Building Engineering},
  volume   = {6},
  pages    = {144-156},
  year     = {2016},
  issn     = {2352-7102},
  doi      = {https://doi.org/10.1016/j.jobe.2016.03.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352710216300262},
  author   = {Muhammad Bilal and Lukumon O. Oyedele and Olugbenga O. Akinade and Saheed O. Ajayi and Hafiz A. Alaka and Hakeem A. Owolabi and Junaid Qadir and Maruf Pasha and Sururah A. Bello},
  keywords = {Construction waste, Big data analytics, Building information modelling (BIM), Design optimisation, Construction waste analytics, Waste prediction and minimisation},
  abstract = {In recent times, construction industry is enduring pressure to take drastic steps to minimise waste. Waste intelligence advocates retrospective measures to manage waste after it is produced. Existing waste intelligence based waste management software are fundamentally limited and cannot facilitate stakeholders in controlling wasteful activities. Paradoxically, despite a great amount of effort, the waste being produced by the construction industry is escalating. This undesirable situation motivates a radical change from waste intelligence to waste analytics (in which waste is propose to be tackle proactively right at design through sophisticated big data technologies). This paper highlight that waste minimisation at design (a.k.a. designing-out waste) is data-driven and computationally intensive challenge. The aim of this paper is to propose a Big Data architecture for construction waste analytics. To this end, existing literature on big data technologies is reviewed to identify the critical components of the proposed Big Data based waste analytics architecture. At the crux, graph-based components are used: in particular, a graph database (Neo4J) is adopted to store highly voluminous and diverse datasets. To complement, Spark, a highly resilient graph processing system, is employed. Provision for extensions through Building Information Modelling (BIM) are also considered for synergy and greater adoption. This symbiotic integration of technologies enables a vibrant environment for design exploration and optimisation to tackle construction waste. The main contribution of this paper is that it presents, to the best of our knowledge, the first Big Data based architecture for construction waste analytics. The architecture is validated for exploratory analytics of 200,000 waste disposal records from 900 completed projects. It is revealed that existing waste management software classify the bulk of construction waste as mixed waste, which exposes poor waste data management. The findings of this paper will be of interest, more generally to researchers, who are seeking to develop big data based simulation tools in similar non-trivial applications.}
}
@article{CASTIGLIONE2014203,
  title    = {Exploiting mean field analysis to model performances of big data architectures},
  journal  = {Future Generation Computer Systems},
  volume   = {37},
  pages    = {203-211},
  year     = {2014},
  note     = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2013.07.016},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X13001611},
  author   = {Aniello Castiglione and Marco Gribaudo and Mauro Iacono and Francesco Palmieri},
  keywords = {Multi-formalism modeling, Big data, Mean field analysis, Performance evaluation, Map-reduce architectures},
  abstract = {Big data processing systems are characterized by a relevant number of components that are used in parallel to run multiple instances of the same tasks in order to achieve the needed performance levels in applications characterized by huge amounts of data. Such a number of components depend on the dimension of the involved data, so that new resources (e.g., processing or storage servers) are usually added as the working database grows. A reliable performance evaluation of these systems is at the same time crucial, in order to enable administrators and developers to keep the pace with data growth, and extremely difficult, due to the intrinsic complexity of these architectures. Notwithstanding, the available literature does not yet offer sufficient experiences, nor significant methodologies, in such a direction. This paper presents a novel modeling approach, based on mean field analysis, a set of methods for approximate inference of probabilistic models, derived from statistical physics, for performance evaluation of big data systems. This approach, by containing the excessive state space growth characterizing more traditional modeling methodologies, also requires a significantly reduced effort with respect to simulation based ones.}
}
@article{SFAXI2021100186,
  title    = {Babel: A Generic Benchmarking Platform for Big Data Architectures},
  journal  = {Big Data Research},
  volume   = {24},
  pages    = {100186},
  year     = {2021},
  issn     = {2214-5796},
  doi      = {https://doi.org/10.1016/j.bdr.2021.100186},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214579621000034},
  author   = {Lilia Sfaxi and Mohamed Mehdi {Ben Aissa}},
  keywords = {Benchmark, Big Data architectures, Performance analysis, Event-driven systems, Cloud computing, Distributed systems},
  abstract = {In this era of big and fast data, software architects tend to find it really hard to make consistent decisions about which architecture and technologies are ideal for a certain business need. It is even harder to make them while dealing with the scarcity of clear methodologies, best practices and reference architectures. In this prospect, architecture evaluation through benchmarking can be of great interest, as it enables the detection of performance anomalies or bottlenecks as you go. The problem when talking about Big Data benchmarking, is that existing solutions remain technology-related, and do not deal with the heterogeneous aspect of complex architectures. In addition to that, businesses are in general dealing with multi-layered complex systems, involving various technologies, paradigms and micro-architectures. This means that the benchmarking solution must be able to give fine-grained insights about each of the layers. A successful benchmarking system must also be seamless, easy to use, scalable, and preferably cloud native. To satisfy these requirements, we designed and implemented Babel, a generic Big Data benchmarking platform, that insures an end-to-end performance evaluation and monitoring. We present in this paper the principles, architecture, integration and deployment steps of Babel.}
}
@article{MANOGARAN2016128,
  title    = {MetaCloudDataStorage Architecture for Big Data Security in Cloud Computing},
  journal  = {Procedia Computer Science},
  volume   = {87},
  pages    = {128-133},
  year     = {2016},
  note     = {Fourth International Conference on Recent Trends in Computer Science & Engineering (ICRTCSE 2016)},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2016.05.138},
  url      = {https://www.sciencedirect.com/science/article/pii/S187705091630477X},
  author   = {Gunasekaran Manogaran and Chandu Thota and M. Vijay Kumar},
  keywords = {Big Data Architecture, Big Data Security Architecture, Cloud Computing},
  abstract = {The cloud is increasingly being used to store and process the big data. Many researchers have been trying to protect big data in cloud computing environment. Traditional security mechanisms using encryption are neither efficient nor suited to the task of protecting big data in the Cloud. In this paper, we first discuss about challenges and potential solutions for protecting big data in cloud computing. Second, we proposed MetaCloudDataStorage Architecture for protecting Big Data in Cloud Computing Environment. This framework ensures that efficient processing of big data in cloud computing environment and gains more business insights.}
}
@article{FIROUZI2018583,
  title    = {Internet-of-Things and big data for smarter healthcare: From device to architecture, applications and analytics},
  journal  = {Future Generation Computer Systems},
  volume   = {78},
  pages    = {583-586},
  year     = {2018},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2017.09.016},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17319726},
  author   = {Farshad Firouzi and Amir M. Rahmani and K. Mankodiya and M. Badaroglu and G.V. Merrett and P. Wong and Bahar Farahani},
  abstract = {The technology and healthcare industries have been deeply intertwined for quite some time. New opportunities, however, are now arising as a result of fast-paced expansion in the areas of the Internet of Things (IoT) and Big Data. In addition, as people across the globe have begun to adopt wearable biosensors, new applications for individualized eHealth and mHealth technologies have emerged. The upsides of these technologies are clear: they are highly available, easily accessible, and simple to personalize; additionally they make it easy for providers to deliver individualized content cost-effectively, at scale. At the same time, a number of hurdles currently stand in the way of truly reliable, adaptive, safe and efficient personal healthcare devices. Major technological milestones will need to be reached in order to address and overcome those hurdles; and that will require closer collaboration between hardware and software developers and medical personnel such as physicians, nurses, and healthcare workers. The purpose of this special issue is to analyze the top concerns in IoT technologies that pertain to smart sensors for health care applications; particularly applications targeted at individualized tele-health interventions with the goal of enabling healthier ways of life. These applications include wearable and body sensors, advanced pervasive healthcare systems, and the Big Data analytics required to inform these devices.}
}
@article{DHANALAKSHMI2022,
  title    = {Security threats and approaches in E-Health cloud architecture system with big data strategy using cryptographic algorithms},
  journal  = {Materials Today: Proceedings},
  year     = {2022},
  issn     = {2214-7853},
  doi      = {https://doi.org/10.1016/j.matpr.2022.03.254},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214785322015978},
  author   = {G. Dhanalakshmi and Victo Sudha George},
  keywords = {Cloud Storage, Security Issues, Threats, Big Data, Cryptography algorithms},
  abstract = {Cloud computing refers to the use of the Internet to deliver software to cloud consumers. For storing and analyzing huge volumes of data from social media, enterprises, organizations, government sectors, and other sources, cloud infrastructure is essential. As a result, a large storage capacity is required to store all of the data on the cloud. The existing system still has a lot of work to do in terms of improving DDoS attacks, data breaches, and data loss. To address the aforementioned difficulties, many academics have focused on the creation of powerful and sensitive data, the lack of data recovery, and data storage, utilizing cloud protection strategies to capitalize on them. We propose a Big Data E-Healthcare Information System that utilizes cloud platforms to connect hospitals, patients, and health centers in this paper. We eliminate anything with third-party monitoring, enabling hospital and clinic patients to view the same data at the same time. and helps protect static or dynamic protection from hacker’s observation, modification, or interference using RSA Cryptographic Algorithms.}
}
@article{GUERREIRO2019916,
  title    = {A Self-Adapted Swarm Architecture to Handle Big Data for “Factories of the Future”},
  journal  = {IFAC-PapersOnLine},
  volume   = {52},
  number   = {13},
  pages    = {916-921},
  year     = {2019},
  note     = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2019.11.356},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896319313345},
  author   = {Guilherme Guerreiro and Ruben Costa and Paulo Figueiras and Diogo Graça and Ricardo Jardim-Gonçalves},
  keywords = {Swarm Intelligence, Industry4.0, Big Data, Manufacturing Data Processing, Cyber-Physical Systems},
  abstract = {Currently, the manufacturing sector is facing a technological evolution with the so-called Industry 4.0. This poses a paradigm shift, enabling companies to be more competitive by taking advantage of innovative technologies(cloud computing, cyber-physical systems, big data analytics and deep learning), pursuing near-zero fault, near real-time reactivity to any problem, better traceability, more predictability in manufacturing, while working to achieve cheaper product customization. The challenges arise when the dimensionality of the data generated by manufacturing processes grows, affecting the performance of algorithms, decreasing it quickly as the dimension of the search space increases. Handling large datasets with a good performance in a limited time should be the main concern in Big Data analytics. This paper focuses on a logistic process of car manufacturing, where batteries are unloaded from trucks to warehouse, and then to the point of fit, where they are assembled into the car. It presents a complete data-driven architecture, using a swarm approach for distributed data processing among all data stages, where processing nodes with different tasks and technologies can work cooperatively to complete a job. The work presented in this paper is funded by the EU project BOOST4.0, focusing on a smart manufacturing scenario for the automotive sector.}
}
@article{WANG2019160,
  title    = {An integrated GIS platform architecture for spatiotemporal big data},
  journal  = {Future Generation Computer Systems},
  volume   = {94},
  pages    = {160-172},
  year     = {2019},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2018.10.034},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17319283},
  author   = {Shaohua Wang and Yang Zhong and Erqi Wang},
  keywords = {Spatiotemporal big data, Distributed computing framework, Cloud-terminal integration GIS, SuperMap GIS},
  abstract = {With the increase in smart devices, spatiotemporal data has grown exponentially. To deal with challenges caused by an increase data requires a scalable and efficient architecture that can store, query, analyze, and visualize spatiotemporal big data. This paper describes a Cloud-terminal integrated GIS platform architecture designed to meet the requirements of processing and analyzing spatiotemporal big data. Cloud-terminal Integration GIS is developed according to the architecture. Extensive experiments deployed on the internal organization cluster using real-time datasets showed that the SuperMap GIS spatiotemporal big data engine achieved excellent performance.}
}
@article{NADAL201775,
  title    = {A software reference architecture for semantic-aware Big Data systems},
  journal  = {Information and Software Technology},
  volume   = {90},
  pages    = {75-92},
  year     = {2017},
  issn     = {0950-5849},
  doi      = {https://doi.org/10.1016/j.infsof.2017.06.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950584917304287},
  author   = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
  keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
  abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.}
}
@article{OUAFIQ2022102093,
  title    = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
  journal  = {Sustainable Energy Technologies and Assessments},
  volume   = {52},
  pages    = {102093},
  year     = {2022},
  issn     = {2213-1388},
  doi      = {https://doi.org/10.1016/j.seta.2022.102093},
  url      = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
  author   = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
  keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
  abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{SPANGENBERG2017310,
  title    = {A Big Data architecture for intra-surgical remaining time predictions},
  journal  = {Procedia Computer Science},
  volume   = {113},
  pages    = {310-317},
  year     = {2017},
  note     = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2017.08.332},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050917317428},
  author   = {Norman Spangenberg and Moritz Wilke and Bogdan Franczyk},
  keywords = {Remaining Intervention Time Prediction, Big Data Architecture, Predictive Analytics},
  abstract = {The operating room area is still one of the most expensive sections in the hospital due to the high resource requirements and the diverse uncertainties. However there are few solutions that support monitoring and decision-making in operating room management. But with new data sources and analytical methods of big data research more improvements could be achieved. In this work we utilize surgical phase events recognized in surgical device data to learn prediction models and trigger online predictions for remaining intervention times in operating rooms. To identify the best algorithm for prediction model computation with the existing data, we evaluate a set of regression algorithms. Based on this methods we propose an architecture approach for the integrated processing of real-time data and historic learning data. The evaluation and comparison with related work shows that our prototype is competitive regarding prediction accuracy.}
}
@article{ALEXANDREDASILVA2016114,
  title    = {Strategies for Big Data Analytics through Lambda Architectures in Volatile Environments},
  journal  = {IFAC-PapersOnLine},
  volume   = {49},
  number   = {30},
  pages    = {114-119},
  year     = {2016},
  note     = {4th IFAC Symposium on Telematics Applications TA 2016},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2016.11.138},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896316325897},
  author   = {Veith {Alexandre da Silva} and Anjos {Julio C.S. dos} and de Freitas {Edison Pignaton} and Lampoltshammer {Thomas J.} and Geyer {Claudio F.}},
  keywords = {Internet of Things (IoT), Scheduling, Batch processing, Stream processing, Cloud computing, Grid computin},
  abstract = {Abstract:
              Expectations regarding the future growth of Internet of Things (IoT)-related technologies are high. These expectations require the realization of a sustainable general purpose application framework that is capable to handle these kind of environments with their complexity in terms of heterogeneity and volatility. The paradigm of the Lambda architecture features key characteristics (such as, robustness, fault tolerance, scalability, generalization, extensibility, ad-hoc queries, minimal maintenance, and low-latency reads and updates) to cope with this complexity. The paper at hand suggest a basic set of strategies to handle the arising challenges regarding the volatility, heterogeneity, and desired low latency execution by reducing the overall system timing (scheduling, execution, monitoring, and faults recovery) as well as possible faults (churn, no answers to executions). The proposed strategies make use of services such as migration, replication, MapReduce simulation, and combined processing methods (batch- and streaming-based). Via these services, a distribution of tasks for the best balance of computational resources is achieved, while monitoring and management can be performed asynchronously in the background.}
}
@article{BABAR2022100719,
  title    = {Smart teledentistry healthcare architecture for medical big data analysis using IoT-enabled environment},
  journal  = {Sustainable Computing: Informatics and Systems},
  volume   = {35},
  pages    = {100719},
  year     = {2022},
  issn     = {2210-5379},
  doi      = {https://doi.org/10.1016/j.suscom.2022.100719},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210537922000567},
  author   = {Muhammad Babar and Muhammad Usman Tariq and Mohammad Dahman Alshehri and Fasee Ullah and M. Irfan Uddin},
  keywords = {Big Data analytics, Medical data, Smart health, Internet of Things},
  abstract = {The current spread out in Big Data analytics and the medical Internet of Things (IoT) originated the recognition of smart health. Smart health is the integration of devices, sensors, cameras, and objects or things embedded with sensors that generate an enormous amount of data known as big data. IoT-enabled smart health management assimilates the digital traces generated by things and humans including teledentistry. The key purpose of teledentistry is to analyze the data and produce valuable insights and hidden patterns to offer services and overcome the existing challenges. This article proposes a data management design using Big Data analytics for smart teledentistry planning. The proposed scheme is the customization of the present parallel platforms to accomplish effective processing. The design offers three different modules which are the data pre-processing, data ingestion, and data processing module. The pre-processing is performed to speed up the real data processing. The recent schemes lack well-organized and effective parallel data loading and proficient communication. The Big Data storage and ingestion are achieved using optimized utility and parallel mechanisms. Optimized RDD-enabled (resilient distribution) Yet Another Resource Negotiator (YARN) based proposal is designed for resourceful cluster administration and computation of medical Big Data. The proposed data design is experimentally simulated with authentic datasets using Apache Spark 3.0 framework. The proposed architecture is compared with existing state-of-the-art proposals. The simulation results reveal the efficacy of the proposed system.}
}
@article{TAMYM2021102,
  title    = {A big data based architecture for collaborative networks: Supply chains mixed-network},
  journal  = {Computer Communications},
  volume   = {175},
  pages    = {102-111},
  year     = {2021},
  issn     = {0140-3664},
  doi      = {https://doi.org/10.1016/j.comcom.2021.05.008},
  url      = {https://www.sciencedirect.com/science/article/pii/S0140366421001924},
  author   = {Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}},
  keywords = {Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness},
  abstract = {Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.}
}
@article{GUO2015207,
  title    = {An effective and economical architecture for semantic-based heterogeneous multimedia big data retrieval},
  journal  = {Journal of Systems and Software},
  volume   = {102},
  pages    = {207-216},
  year     = {2015},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/j.jss.2014.09.016},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121214002040},
  author   = {Kehua Guo and Wei Pan and Mingming Lu and Xiaoke Zhou and Jianhua Ma},
  keywords = {Heterogeneous multimedia, Semantic based retrieval, Big data},
  abstract = {Data variety has been one of the most critical features for multimedia big data. Some multimedia documents, although in different data formats and storage structures, often express similar semantic information. Therefore, the way to manage and retrieve multimedia documents reflecting users’ intent in heterogeneous big data environments has become an important issue. In this paper, we present an effective and economical architecture named SHMR (Semantic-based Heterogeneous Multimedia Retrieval), which uses low cost to store and retrieve semantic information from heterogeneous multimedia data. Firstly, the particularity of heterogeneous multimedia retrieval in big data environments is addressed. Secondly, an approach to extract and represent semantic information for heterogeneous multimedia documents is proposed. Thirdly, a NoSQL-based approach to semantic storage, in which multimedia can be parallel processed in distributed nodes is provided. Finally, a MapReduce-based retrieval algorithm is presented and a user feedback supported scheme to achieve high retrieval precision and good user experience is designed. The experimental results indicate that the retrieval performance and economic efficiency of SHMR are suitable for multimedia information retrieval in heterogeneous big data environments.}
}
@article{KRAMER201569,
  title    = {A modular software architecture for processing of big geospatial data in the cloud},
  journal  = {Computers & Graphics},
  volume   = {49},
  pages    = {69-81},
  year     = {2015},
  issn     = {0097-8493},
  doi      = {https://doi.org/10.1016/j.cag.2015.02.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0097849315000138},
  author   = {Michel Krämer and Ivo Senner},
  keywords = {Cloud computing, Big Data, Geoprocessing, Distributed systems, Software architectures, Domain-specific languages},
  abstract = {In this paper we propose a software architecture that allows for processing of large geospatial data sets in the cloud. Our system is modular and flexible and supports multiple algorithm design paradigms such as MapReduce, in-memory computing or agent-based programming. It contains a web-based user interface where domain experts (e.g. GIS analysts or urban planners) can define high-level processing workflows using a domain-specific language (DSL). The workflows are passed through a number of components including a parser, interpreter, and a service called job manager. These components use declarative and procedural knowledge encoded in rules to generate a processing chain specifying the execution of the workflows on a given cloud infrastructure according to the constraints defined by the user. The job manager evaluates this chain, spawns processing services in the cloud and monitors them. The services communicate with each other through a distributed file system that is scalable and fault-tolerant. Compared to previous work describing cloud infrastructures and architectures we focus on the processing of big heterogeneous geospatial data. In addition to that, we do not rely on only one specific programming model or a certain cloud infrastructure but support several ones. Combined with the possibility to control the processing through DSL-based workflows, this makes our architecture very flexible and configurable. We do not only see the cloud as a means to store and distribute large data sets but also as a way to harness the processing power of distributed computing environments for large-volume geospatial data sets. The proposed architecture design has been developed for the IQmulus research project funded by the European Commission. The paper concludes with the evaluation results from applying our solution to two example workflows from this project.}
}
@incollection{AVCISALMA201749,
  title     = {Chapter 4 - Domain-Driven Design of Big Data Systems Based on a Reference Architecture},
  editor    = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
  booktitle = {Software Architecture for Big Data and the Cloud},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {49-68},
  year      = {2017},
  isbn      = {978-0-12-805467-3},
  doi       = {https://doi.org/10.1016/B978-0-12-805467-3.00004-1},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128054673000041},
  author    = {Cigdem {Avci Salma} and Bedir Tekinerdogan and Ioannis N. Athanasiadis},
  keywords  = {Feature driven design, Feature modeling, Big data, Reference architecture, Domain analysis},
  abstract  = {In general, different application domains may require different big data systems. To enhance the understanding of big data systems and support the architect in designing big data architectures, we propose a domain-driven design approach for deriving application architectures. To this end, we propose a domain engineering approach in which a family feature model, reference architecture, and corresponding design rules are identified. The family feature model is derived based on a domain analysis of big data systems and represents the common and variant features. The reference architecture represents a generic structure for various application architectures of big data systems. Finally, the design rules define reusable design heuristics for designing an application architecture based on the selection of features of the family feature model and the reference architecture. We illustrate our approach for deriving the big data architectures of different well-known big data systems.}
}
@article{JAYASENA2017135,
  title    = {Multi-modal Multimedia Big Data Analyzing Architecture and Resource Allocation on Cloud Platform},
  journal  = {Neurocomputing},
  volume   = {253},
  pages    = {135-143},
  year     = {2017},
  note     = {Learning Multimodal Data},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2016.11.077},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231217304411},
  author   = {K.P.N. Jayasena and Lin Li and Qing Xie},
  keywords = {Multi-modal, Multimedia big data, Resource allocation, Cloud computing, Hadoop & MapReduce, ACO},
  abstract = {Multimedia big data analyzing is the new topic that focus on all features of distributed computing systems that contains of a combination of text, visual and audio modalities. The traditional method to transcoding multi-modal multimedia big data needs expensive hardware and the amount of data increases transcoding executes a significant burden on the computing infrastructure. Therefore we illustrate a novel implementation for multimedia big data analyzing and data distribution. Our proposed architecture contains three layers such as service layer, platform layer and infrastructure layer. We design and implement the platform layer of the system by using a MapReduce framework running on a hadoop distributed file system (HDFS) and the media processing libraries Xuggler. In this way, our proposed system reduces the time for transcoding large amounts of data into specific formats depending on the user requirements. It provides flexible multimedia record/write interface and we can build large scale multimedia big data analytic applications based on Hadoop cloud platform. Moreover, we proposed the ant colony optimization (ACO) algorithm for efficient resource allocation in infrastructure layer. The simulation results demonstrate that the proposed algorithm can optimally allocate VM to achieve a minimal response time.}
}
@incollection{PANNEERSELVAM2015131,
  title     = {Chapter 9 - Requirements and Challenges for Big Data Architectures},
  editor    = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
  booktitle = {Application of Big Data for National Security},
  publisher = {Butterworth-Heinemann},
  pages     = {131-139},
  year      = {2015},
  isbn      = {978-0-12-801967-2},
  doi       = {https://doi.org/10.1016/B978-0-12-801967-2.00009-4},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128019672000094},
  author    = {John Panneerselvam and Lu Liu and Richard Hill},
  keywords  = {Acquisition, Analysis, Deployment, Extract, Integration, Organization},
  abstract  = {Big data analytics encompass the integration of a range of techniques while deploying and using this technology in practice. The processing requirements of big data span across multiple machines with the seamless integration of a large range of subsystems functioning in a distributed fashion. Because we witness the processing requirements of big data exceeding the capabilities of most current machines, building up such a huge processing architecture involves several challenges in terms of both the hardware and software deployments. Understanding the analytics assets across the underpinning core technologies will add additional values, while driving innovation and insights with big data. This chapter is structured to showcase the challenges and requirements of a big data processing infrastructure and also gives an overview of the core technologies and considerations involved in the concept of big data processing.}
}
@article{SILVA2020975,
  title    = {Integration of Big Data analytics embedded smart city architecture with RESTful web of things for efficient service provision and energy management},
  journal  = {Future Generation Computer Systems},
  volume   = {107},
  pages    = {975-987},
  year     = {2020},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2017.06.024},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17305174},
  author   = {Bhagya Nathali Silva and Murad Khan and Kijun Han},
  keywords = {Smart city, Big Data analytics, Smart home, Web of things, RESTful architecture},
  abstract = {Emergence of smart things has revolutionized the conventional internet into a connected network of things, maturing the concept of Internet of Things (IoT). With the evolution of IoT, many attempts were made to realize the notion of smart cities. However, demands for processing enormous amount of data and platform incompatibilities of connected smart things hindered the actual implementation of smart cities. Keeping it in view, we proposed a Big Data analytics embedded smart city architecture, which is further integrated with the web via a smart gateway. Integration with the web provides a universal communication platform to overcome the platform incompatibilities of smart things. We introduced Big Data analytics to enhance data processing speed. Further, we evaluated authentic datasets to determine the threshold values for intelligent decision-making and to present the performance improvement gained in data processing. Finally, we presented a representational state transfer (RESTful) web of things (WoT) integrated smart building architecture (smart home) to reveal the performance improvements of the proposed smart city architecture in terms of network performance and energy management of smart buildings.}
}
@incollection{KRISHNAN2013219,
  title     = {Chapter 11 - Data-Driven Architecture for Big Data},
  editor    = {Krish Krishnan},
  booktitle = {Data Warehousing in the Age of Big Data},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {219-240},
  year      = {2013},
  series    = {MK Series on Business Intelligence},
  isbn      = {978-0-12-405891-0},
  doi       = {https://doi.org/10.1016/B978-0-12-405891-0.00011-8},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124058910000118},
  author    = {Krish Krishnan},
  keywords  = {metadata, master data, machine learning, algorithms, semantic libraries, data governance},
  abstract  = {The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.}
}
@article{MOSTEFAOUI2022374,
  title    = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
  journal  = {Future Generation Computer Systems},
  volume   = {134},
  pages    = {374-387},
  year     = {2022},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2022.04.020},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
  author   = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
  keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
  abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{GUO2021100219,
  title    = {Study on Landscape Architecture Model Design Based on Big Data Intelligence},
  journal  = {Big Data Research},
  volume   = {25},
  pages    = {100219},
  year     = {2021},
  issn     = {2214-5796},
  doi      = {https://doi.org/10.1016/j.bdr.2021.100219},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214579621000368},
  author   = {Shiyun Guo and Jinping Tang and Huabin Liu and Xinren Gu},
  keywords = {Big data, Cluster analysis, Landscape architecture, Programming, Parameter model construction, Geographic information systems},
  abstract = {Because of the rapid development of Internet technology in recent years, the speed of information data growth is faster and faster, through the use of Internet information data to landscape architecture analysis and research, has become the main direction of industry development. Landscape planners are involved in a wide range of projects, and architectural objects are often very complex in layout. Projects are usually composed of multiple components. For the point clouds corresponding to these objects, the direct reconstruction of them is relatively complex, which requires macroscopic and reasonable planning on the regional scale. Generally, plots should be designed in three-dimensional space. The contents presented by planning and design of different scales are also different. In dealing with multi-level regional planning and design, this paper mainly focuses on the parametric design technology research of landscape architecture. First, it starts from the design of large areas, analyzes the status quo of regional scale, and then starts to design with the help of three-dimensional model. First will get the clusters of 3 d model the coarse segmentation class continue to split into more rules part, according to the characteristics of the model data, the first of 3 d model to differential information to estimate the point cloud data, calculate the normal vector and curvature of point cloud data, through a point cloud registration technology will point cloud unified under different Angle of view to the same coordinate system, through the big data landscape algorithm for geometric feature and image feature point detection, detailed point cloud data processing algorithms in the process of coarse segmentation of regional scale in the same cluster of different object segmentation, again through the panoramic image segmentation for interior point cloud of geometric structure information. The data can be classified intelligently, the corresponding point cloud data can be marked, and the geometric structure information can be marked into the point cloud data to be segmented through point cloud matching, so as to enhance knowledge reserves, find problems and solve problems timely.}
}
@article{MARTINEZPRIETO201562,
  title    = {The Solid  architecture for real-time management of big semantic data},
  journal  = {Future Generation Computer Systems},
  volume   = {47},
  pages    = {62-79},
  year     = {2015},
  note     = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2014.10.016},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X1400209X},
  author   = {Miguel A. Martínez-Prieto and Carlos E. Cuesta and Mario Arias and Javier D. Fernández},
  keywords = {Tiered architecture, Big semantic data, Real-time,  triplestores},
  abstract = {Big Data management has become a critical task in many application systems, which usually rely on heavyweight batch processes to manage such large amounts of data. However, batch architectures are not an adequate choice for designing real-time systems in which data updates and reads must be satisfied with very low latency. Thus, gathering and consuming high volumes of data at high velocities is an emerging challenge which we specifically address in the scope of innovative scenarios based on semantic data (RDF) management. The Linked Open Data initiative or emergent projects in the Internet of Things are examples of such scenarios. This paper describes a new architecture (referred to as Solid) which separates the complexities of Big Semantic Data storage and indexing from real-time data acquisition and consumption. This decision relies on the use of two optimized datastores which respectively store historical (big) data and run-time data. It ensures efficient volume management and high processing velocity, but adds the need of coordinating both datastores. Solid  proposes a 3-tiered architecture in which each responsibility is specifically addressed. Besides its theoretical description, we also propose and evaluate a Solid  prototype built on top of binary RDF and state-of-the-art triplestores. Our experimental numbers report that Solid  achieves large savings in data storage (it uses up to 5 times less space than the compared triplestores), while provides efficient SPARQL resolution over the Big Semantic Data (in the order of 10–20 ms for the studied queries). These experiments also show that Solid  ensures low-latency operations because data effectively managed in real-time remain small, so do not suffer Big Data issues.}
}
@incollection{KRISHNAN201329,
  title     = {Chapter 3 - Big Data Processing Architectures},
  editor    = {Krish Krishnan},
  booktitle = {Data Warehousing in the Age of Big Data},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {29-43},
  year      = {2013},
  series    = {MK Series on Business Intelligence},
  isbn      = {978-0-12-405891-0},
  doi       = {https://doi.org/10.1016/B978-0-12-405891-0.00003-9},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124058910000039},
  author    = {Krish Krishnan},
  keywords  = {network, distance, communication, dedicated resources, horizontal partitioning, vertical partitioning},
  abstract  = {The first two chapters provided you an introduction to Big Data and the complexities associated with Big Data. This chapter deals with the architectures that are needed or designed to process Big Data and the different techniques that can be adopted for processing vast amounts of data.}
}
@article{LOPEZMARTINEZ2021263,
  title    = {A big data-centric architecture metamodel for Industry 4.0},
  journal  = {Future Generation Computer Systems},
  volume   = {125},
  pages    = {263-284},
  year     = {2021},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2021.06.020},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X21002156},
  author   = {Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla},
  keywords = {Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel},
  abstract = {The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.}
}
@article{ZHANG20171190,
  title    = {Private Colleges and Universities Open Cloud Architecture of Oracle Big Data Laboratory Program},
  journal  = {Procedia Engineering},
  volume   = {174},
  pages    = {1190-1195},
  year     = {2017},
  note     = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
  issn     = {1877-7058},
  doi      = {https://doi.org/10.1016/j.proeng.2017.01.279},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877705817302795},
  author   = {Qian Zhang},
  keywords = {Open Cloud Architecture, Big Data, Oracle Database, Private Universities},
  abstract = {Big data are the most concerned topic nowadays in all walks of life. How to obtain the commercial value through the vast amounts of data, how to transport the big data talents for the enterprise is an urgent concern of the various colleges and universities. In order to improve the employment competitiveness of the students, private colleges and universities develop big data talent training plan and implementation, which is an urgent problem to be solved in the computer major. This paper puts forward the construction and opening of the cloud architecture of the Oracle big data laboratory, and to carry out big data related knowledge training to the computer majors. The article expounds the architecture of Oracle big data solution, the planning and deployment of big data labs. Through the training platform of the laboratory, it cultivates the big data talents with competitive strength for the private colleges and universities.}
}
@article{RAHMAN201629,
  title    = {A Hybrid Data Center Architecture for Big Data},
  journal  = {Big Data Research},
  volume   = {3},
  pages    = {29-40},
  year     = {2016},
  note     = {Special Issue on Big Data from Networking Perspective},
  issn     = {2214-5796},
  doi      = {https://doi.org/10.1016/j.bdr.2016.02.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214579616300120},
  author   = {Mohammad Naimur Rahman and Amir Esmailpour},
  keywords = {Data center networking, Cloud Computing, Big Data, Electrical and optical packet switching},
  abstract = {In the past few years, Big Data analytics have changed the way computing services and resources are being used. New users are getting into the cloud services provided by data centers on a daily basis, and the number of applications hosted on the clouds is rapidly increasing. However, existing data center architectures cannot cope with the black-box nature of the cloud nor with Big Data analytics. Current data center networks using electrical packet switches have many disadvantages. Electrical links consume too much power to handle the increased bandwidth demanded by emerging applications. They are often not suitable for providing high bandwidth at low latency and have bottleneck issues. A promising solution involves adding optical interconnections to the infrastructure in data centers offering high throughput, low latency, and incredibly low energy consumption compared to the current data center networks based on commodity switches. In this work, we propose a hybrid electrical and optical networking architecture for data centers hosting Cloud Computing and Big Data applications. Furthermore, we analyze and compare application behaviors with current and proposed hybrid architectures. The results show that hybrid architecture provides a viable solution to data center issues and easily outperforms existing data center architectures. The results reflect that hybrid architecture reduces the delay by nearly 39% and decreases the cost of cooling systems from 49.57% to 27% of the total costs, while reducing the overall long-term costs of data centers.}
}
@article{SOUZA20151010,
  title    = {An Outlier Detect Algorithm using Big Data Processing and Internet of Things Architecture},
  journal  = {Procedia Computer Science},
  volume   = {52},
  pages    = {1010-1015},
  year     = {2015},
  note     = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2015.05.095},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050915008959},
  author   = {Alberto M.C. Souza and Joseé R.A. Amazonas},
  keywords = {Internet of Things, Big Data, Architecture, Outlier},
  abstract = {Data management in the Internet of Things is a crucial aspect. Considering a world of interconnected objects which constantly exchange many kinds of information, the volume of generated data and involved processes, implies that data management becomes critical. The aim of this paper is to propose an outlier detection procedure using the K-means algorithm and Big Data processing using the Hadoop platform and Mahout implementation integrated with our chosen Internet of Things architecture.}
}
@article{GONCALVES2017585,
  title    = {Towards of a Real-time Big Data Architecture to Intensive Care},
  journal  = {Procedia Computer Science},
  volume   = {113},
  pages    = {585-590},
  year     = {2017},
  note     = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2017.08.294},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050917317039},
  author   = {André Gonçalves and Filipe Portela and Manuel Filipe Santos and Fernando Rua},
  keywords = {Intensive Medicine, Intensive Care Units, Real-time, Big Data, Architecture, Hadoop},
  abstract = {These days the exponential increase in the volume and variety of data stored by companies and organizations of various sectors of activity, has required to organizations the search for new solutions to improve their services and/or products, taking advantage of technological evolution. As a response to the inability of organizations to process large quantities and varieties of data, in the technological market, arise the Big Data. This emerging concept defined mainly by the volume, velocity and variety has evolved greatly in part by its ability to generate value for organizations in decision making. Currently, the health care sector is one of the five sectors of activity where the potential of Big Data growth most stands out. However, the way to go is still long and in fact there are few organizations, related to health care, that are taking advantage of the true potential of Big Data. The main target of this research is to produce a real-time Big Data architecture to the INTCare system, of the Centro Hospitalar do Porto, using the main open source big data solution, the Apache Hadoop. As a result of the first phase of this research we obtained a generic architecture who can be adopted by other Intensive Care Units.}
}
@article{HADJSASSI2019534,
  title    = {A New Architecture for Cognitive Internet of Things and Big Data},
  journal  = {Procedia Computer Science},
  volume   = {159},
  pages    = {534-543},
  year     = {2019},
  note     = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2019.09.208},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
  author   = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
  keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
  abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{LI2017180,
  title    = {Composable architecture for rack scale big data computing},
  journal  = {Future Generation Computer Systems},
  volume   = {67},
  pages    = {180-193},
  year     = {2017},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2016.07.014},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X16302631},
  author   = {Chung-Sheng Li and Hubertus Franke and Colin Parris and Bulent Abali and Mukil Kesavan and Victor Chang},
  keywords = {Big data platforms, Composable system architecture, Disaggregated datacenter architecture, Composable datacenter, Software defined environments, Software defined networking},
  abstract = {The rapid growth of cloud computing, both in terms of the spectrum and volume of cloud workloads, necessitates re-visiting the traditional rack-mountable servers based datacenter design. Next generation datacenters need to offer enhanced support for: (i) fast changing system configuration requirements due to workload constraints, (ii) timely adoption of emerging hardware technologies, and (iii) maximal sharing of systems and subsystems in order to lower costs. Disaggregated datacenters, constructed as a collection of individual resources such as CPU, memory, disks etc., and composed into workload execution units on demand, are an interesting new trend that can address the above challenges. In this paper, we demonstrate the feasibility of composable systems through building a rack scale composable system prototype using PCIe switches. Through empirical approaches, we develop an assessment of the opportunities and challenges for leveraging the composable architecture for rack scale cloud datacenters with a focus on big data and NoSQL workloads. In particular, we compare and contrast the programming models that can be used to access the composable resources, and develop the implications for the network and resource provisioning and management for rack scale architecture.}
}
@article{KASTOUNI2020,
  title    = {Big data analytics in telecommunications: Governance, architecture and use cases},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  year     = {2020},
  issn     = {1319-1578},
  doi      = {https://doi.org/10.1016/j.jksuci.2020.11.024},
  url      = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
  author   = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
  keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
  abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{LUO2016147,
  title    = {A new big data storage architecture with intrinsic search engines},
  journal  = {Neurocomputing},
  volume   = {181},
  pages    = {147-152},
  year     = {2016},
  note     = {Big Data Driven Intelligent Transportation Systems},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2015.06.103},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231215019074},
  author   = {Jianjun Luo and Lingyan Fan and Zhenhua Li and Chris Tsu},
  keywords = {Big data, Data storage, SSD, iSearch},
  abstract = {The data storage system is central in determining the performance and cost in data mining or ITS. As the computing power of servers has increased so have the problems caused by the bottlenecks from slower storage protocol interfaces, which restrict data throughput and the accessing raw data from the physical storage systems. This paper presented new big data storage architecture to optimize the efficiency of data mining or mass surveillance by integrating a distributed and embedded searching engine inside each storage drive. By integrating the intrinsic search engine (iSearch) into the core controller chip some of the work of searching for patterns and keywords takes place inside the drive freeing up resources of a higher level host and ultimately the server. Only those drives, in which the expected pattern or keywords were detected, are analyzed by the higher level host. Not only does iSearch free up the server for other high level computing tasks it also helps preserve as the bandwidth of the big data storage interface.}
}
@article{FAHMIDEH2019948,
  title    = {Big data analytics architecture design—An application in manufacturing systems},
  journal  = {Computers & Industrial Engineering},
  volume   = {128},
  pages    = {948-963},
  year     = {2019},
  issn     = {0360-8352},
  doi      = {https://doi.org/10.1016/j.cie.2018.08.004},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360835218303760},
  author   = {Mahdi Fahmideh and Ghassan Beydoun},
  keywords = {Big data, Big data analytics platforms, Manufacturing systems, Goal-oriented modelling, Fuzzy logic, Data analytics architecture},
  abstract = {Context
              The rapid prevalence and potential impact of big data analytics platforms have sparked an interest amongst different practitioners and academia. Manufacturing organisations are particularly well suited to benefit from data analytics platforms in their entire product lifecycle management for intelligent information processing, performing manufacturing activities, and creating value chains. This needs a systematic re-architecting approach incorportaitng careful and thorough evaluation of goals for integrating manufacturing legacy information systems with data analytics platforms. Furthermore, ameliorating the uncertainty of the impact the new big data architecture on system quality goals is needed to avoid cost blowout in implementation and testing phases.
              Objective
              We propose an approach for goal-obstacle analysis and selecting suitable big data solution architectures that satisfy quality goal preferences and constraints of stakeholders at the presence of the decision outcome uncertainty. The approach will highlight situations that may impede the goals. They will be assessed and resolved to generate complete requirements of an architectural solution.
              Method
              The approach employs goal-oriented modelling to identify obstacles causing quality goal failure and their corresponding resolution tactics. Next, it combines fuzzy logic to explore uncertainties in solution architectures and to find an optimal set of architectural decisions for the big data enablement process of manufacturing systems.
              Result
              The approach brings two innovations to the state of the art of big data analytics platform adoption in manufacturing systems: (i) A goal-oriented modelling for exploring goals and obstacles in integrating systems with data analytics platforms at the requirement level and (ii) An analysis of the architectural decisions under uncertainty. The efficacy of the approach is illustrated with a scenario of reengineering a hyper-connected manufacturing collaboration system to a new big data architecture.}
}
@article{RAO20152774,
  title    = {Big Data Analytics Performance for Large Out-of-Core Matrix Solvers on Advanced Hybrid Architectures},
  journal  = {Procedia Computer Science},
  volume   = {51},
  pages    = {2774-2778},
  year     = {2015},
  note     = {International Conference On Computational Science, ICCS 2015},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2015.05.432},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050915012405},
  author   = {Raghavendra Shruti Rao and Milton Halem and John Dorband},
  keywords = {Matrix Multiplication, Out-Of-Core Matrices, Hybrid Architectures, Phi Co Processors, Tesla GPU},
  abstract = {This paper examines the performance of advanced computer architectures for large Out-Of-Core matrices to assess the optimal Big Data system configurations., The performance evaluation is based on a large dense Lower-Upper Matrix Decomposition (LUD) employing a highly tuned, I/O managed, slab based LUD software package developed by the Lockheed Martin Corporation. We present extensive benchmark studies conducted with this package on UMBC's Bluegrit and Bluewave clusters, and NASA-GFSC's Discover cluster systems. Our results show the speedup for a single node achieved by Phi Co-Processors relative to the host CPU SandyBridge processor is about a 1.5X improvement, which is an even smaller relative performance gain compared with the studies by F. Masci where he obtains a 2-2.5x performance. Surprisingly, the Westmere with the Tesla GPU scales comparably with the Sandy Bridge and the Phi Co-Processor up to 12 processes and then fails to continue to scale. The performances across 20 CPU nodes of SandyBridge obtains a uniform speedup of 0.5X over Westmere for problem sizes of 10K, 20K and 40K unknowns. With an Infiniband DDR, the performance of Nehalem processors is comparable to Westmere without the interconnect.}
}
@article{MP2022102668,
  title    = {Intrusion detection in big data using hybrid feature fusion and optimization enabled deep learning based on spark architecture},
  journal  = {Computers & Security},
  volume   = {116},
  pages    = {102668},
  year     = {2022},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2022.102668},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404822000670},
  author   = {Ramkumar M .P .  and P.V. Bhaskar Reddy and J.T. Thirukrishna and Ch. Vidyadhari},
  keywords = {Big data, Deep learning, Intrusion detection, Deep residual network, and Spark model},
  abstract = {With the rapid expansion of Internet services and increasing intrusion issues, conventional intrusion detection techniques cannot work well with several difficult intrusions. Though several intrusion detection methods have been introduced in the past years, developing an Intrusion Detection Systems (IDS) with prevailing intrusion detection strategy is still very much desirable. Hence, a robust and effective intrusion detection approach, named RV coefficient+Exponential Sea Lion Optimization-enabled Deep Residual Network (ExpSLO-enabled DRN) using spark is devised for the intrusion detection. Here, the unique features are selected using proposed RV coefficient-based hybrid feature fusion, which is designed by the incorporation of wrapper, class-wise information gain (CIG), and Canberra distance in slave node. With the distinctive features selected, the process of data augmentation is done using oversampling for making the data more appropriate to perform the further process in the slave node. Moreover, DRN classifier is utilized for detecting the intrusions in the master node where the DRN training is done using devised ExpSLO algorithm, which is the hybridization of Exponentially Weighted Moving average (EWMA) and Sea Lion Optimization (SLnO). Furthermore, the devised method obtained better performance by considering the evaluation metrics, such as precision, recall, and F-measure with the higher values of 0.8800, 0.8845, and 0.8822 based on without attacks using dataset-2.}
}
@article{MISHRA2018290,
  title    = {ICABiDAS: Intuition Centred Architecture for Big Data Analysis and Synthesis},
  journal  = {Procedia Computer Science},
  volume   = {123},
  pages    = {290-294},
  year     = {2018},
  note     = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2018.01.045},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050918300462},
  author   = {Amit Kumar Mishra},
  keywords = {cognitive architecture, bio-inspired, intuition, synthesis, Big-Data},
  abstract = {Humans are expert in the amount of sensory data they deal with each moment. Human brain not only "analyses" these data but also starts "synthesizing" new information from the existing data. The current age Big-data systems are needed not just to analyze data but also to come up new interpretation. We believe that the pivotal ability in human brain which enables us to do this is what is known as "intuition". Here, we present an intuition based architecture for big data analysis and synthesis.}
}
@incollection{HOQUANG202013,
  title     = {Chapter 2 - Challenges and directions for a community infrastructure for Big Data-driven research in software architecture},
  editor    = {Bedir Tekinerdogan and Önder Babur and Loek Cleophas and Mark {van den Brand} and Mehmet Akşit},
  booktitle = {Model Management and Analytics for Large Scale Systems},
  publisher = {Academic Press},
  pages     = {13-35},
  year      = {2020},
  isbn      = {978-0-12-816649-9},
  doi       = {https://doi.org/10.1016/B978-0-12-816649-9.00010-7},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128166499000107},
  author    = {Truong Ho-Quang and Michel R.V. Chaudron and Regina Hebig and Gregorio Robles},
  keywords  = {software models, software repositories, UML, scientific workflow management, architecture knowledge discovery},
  abstract  = {Research into software architecture and design has become more and more prominent since the 1990s. Since then, companies have reported how software architecting helped them to tackle various challenges in system design, especially related to system-level quality properties such as scalability and maintainability. Academic research in software architecture has focused on several areas, including architecture description through views and architecture description languages, and on methods for evaluating architectural designs. While much of the contribution of research in software architecture was inspired by industrial experiences, little of the research was validated beyond individual case studies. Many scientific disciplines are currently harvesting fruits from large-scale data collection about their subjects of study. Therefore, this chapter contributes a discussion of challenges and directions for Big Data-driven studies of software architecture. Given the large amount of effort that is needed for this type of research, a promising direction is to look into a community-based infrastructure for enabling and supporting this type of research. We share lessons learned through building various tools that could form building blocks in such an infrastructure. Based on these, we synthesize a reference architecture for creating such a community-wide infrastructure for Big Data-based research in software architecture.}
}
@article{WANG2020119299,
  title    = {Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits},
  journal  = {Journal of Cleaner Production},
  volume   = {248},
  pages    = {119299},
  year     = {2020},
  issn     = {0959-6526},
  doi      = {https://doi.org/10.1016/j.jclepro.2019.119299},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652619341691},
  author   = {Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu},
  keywords = {multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration},
  abstract = {Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.}
}
@incollection{BAHSOON20171,
  title     = {Chapter 1 - Introduction. Software Architecture for Cloud and Big Data: An Open Quest for the Architecturally Significant Requirements},
  editor    = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
  booktitle = {Software Architecture for Big Data and the Cloud},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {1-10},
  year      = {2017},
  isbn      = {978-0-12-805467-3},
  doi       = {https://doi.org/10.1016/B978-0-12-805467-3.00001-6},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128054673000016},
  author    = {Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim and Ivan Mistrik},
  keywords  = {Cloud computing, Big data, Software architecture, Architecturally significant requirements},
  abstract  = {We quest for cloud architecturally significant requirements and explain their implications on architecting for/in the cloud in the presence of big data. Awareness of these requirements can help architects and practitioners to formulate architecture design decisions and choices, which are cloud-explicit. We hope to provide insights that can help in the systematic architecting for cloud-based systems and lessen generality and ad hoc practices.}
}
@article{DUTTA2015145,
  title   = {Special issue on software architectures and systems for Big data},
  journal = {Journal of Systems and Software},
  volume  = {102},
  pages   = {145},
  year    = {2015},
  issn    = {0164-1212},
  doi     = {https://doi.org/10.1016/j.jss.2015.01.023},
  url     = {https://www.sciencedirect.com/science/article/pii/S0164121215000175},
  author  = {Kaushik Dutta}
}
@article{PAAKKONEN2015166,
  title    = {Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems},
  journal  = {Big Data Research},
  volume   = {2},
  number   = {4},
  pages    = {166-186},
  year     = {2015},
  issn     = {2214-5796},
  doi      = {https://doi.org/10.1016/j.bdr.2015.01.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214579615000027},
  author   = {Pekka Pääkkönen and Daniel Pakkala},
  keywords = {Big data, Reference architecture, Classification, Literature survey},
  abstract = {Many business cases exploiting big data have been realised in recent years; Twitter, LinkedIn, and Facebook are examples of companies in the social networking domain. Other big data use cases have focused on capturing of value from streaming of movies (Netflix), monitoring of network traffic, or improvement of processes in the manufacturing industry. Also, implementation architectures of the use cases have been published. However, conceptual work integrating the approaches into one coherent reference architecture has been limited. The contribution of this paper is technology independent reference architecture for big data systems, which is based on analysis of published implementation architectures of big data use cases. An additional contribution is classification of related implementation technologies and products/services, which is based on analysis of the published use cases and survey of related work. The reference architecture and associated classification are aimed for facilitating architecture design and selection of technologies or commercial solutions, when constructing big data systems.}
}
@article{CHALVATZIS2019381,
  title    = {Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation},
  journal  = {Technological Forecasting and Social Change},
  volume   = {144},
  pages    = {381-393},
  year     = {2019},
  issn     = {0040-1625},
  doi      = {https://doi.org/10.1016/j.techfore.2018.04.031},
  url      = {https://www.sciencedirect.com/science/article/pii/S0040162517315147},
  author   = {Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary},
  keywords = {Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming},
  abstract = {Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.}
}
@article{NAVAZ2018137,
  title    = {Towards an efficient and Energy-Aware mobile big health data architecture},
  journal  = {Computer Methods and Programs in Biomedicine},
  volume   = {166},
  pages    = {137-154},
  year     = {2018},
  issn     = {0169-2607},
  doi      = {https://doi.org/10.1016/j.cmpb.2018.10.008},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169260718305601},
  author   = {Alramzana Nujum Navaz and Mohamed Adel Serhani and Nabeel Al-Qirim and Marton Gergely},
  keywords = {Analytics customization, Processing, M-health, Mobile big data, Mobile offloading, Resources optimization},
  abstract = {Background and objectives
              Mobile and ubiquitous devices are everywhere, generating an exorbitant amount of data. New generations of healthcare systems are using mobile devices to continuously collect large amounts of different types of data from patients with chronic diseases. The challenge with such Mobile Big Data in general, is how to meet the growing performance demands of the mobile resources handling these tasks, while simultaneously minimizing their consumption.
              Methods
              This research proposes a scalable architecture for processing Mobile Big Data. The architecture is developed around three new algorithms for the effective use of resources in performing mobile data processing and analytics: mobile resources optimization, mobile analytics customization, and mobile offloading. The mobile resources optimization algorithm monitors resources and automatically switches off unused network connections and application services whenever resources are limited. The mobile analytics customization algorithm attempts to save energy by customizing the analytics processes through the implementation of some data-aware schemes. Finally, the mobile offloading algorithm uses some heuristics to intelligently decide whether to process data locally, or delegate it to a cloud back-end server.
              Results
              The three algorithms mentioned above are tested using Android-based mobile devices on real Electroencephalography (EEG) data streams retrieved from sensors and an online data bank. Results show that the three combined algorithms proved their effectiveness in optimizing the resources of mobile devices in handling, processing, and analyzing EEG data.
              Conclusion
              We developed an energy-efficient model for Mobile Big Data which addressed key limitations in mobile device processing and analytics and reduced execution time and limited battery resources. This was supported with the development of three new algorithms for the effective use of resources, energy saving, parallel processing and analytics customization.}
}
@article{LNENICKA2019124,
  title    = {Developing a government enterprise architecture framework to support the requirements of big and open linked data with the use of cloud computing},
  journal  = {International Journal of Information Management},
  volume   = {46},
  pages    = {124-141},
  year     = {2019},
  issn     = {0268-4012},
  doi      = {https://doi.org/10.1016/j.ijinfomgt.2018.12.003},
  url      = {https://www.sciencedirect.com/science/article/pii/S0268401218305942},
  author   = {Martin Lnenicka and Jitka Komarkova},
  keywords = {Government enterprise architecture framework, Design science research, Big data, Open linked data, Cloud computing, Quality attributes, ATAM, Methodology},
  abstract = {Governmental and local authorities are facing many new information and communication technologies challenges. The amount of data is rapidly increasing. The data sets are published in different formats. New services are based on linking and processing differently structured data from various sources. Users expect openness of public data, fast processing, and intuitive visualisation. The article addresses the challenges and proposes a new government enterprise architecture framework. The following partial architectures are included: big and open linked data storage, processing, and publishing using cloud computing. At first, the key concepts are defined. Next, the basic architectural roles and components are specified. The components result from the decomposition of related frameworks. The main part of the article deals with the detailed proposal of the architecture framework and partial views on architecture (sub-architectures). A methodology, including a proposal of appropriate steps, solutions and responsibilities for them, is described in the next step - after the verification and validation of the new framework with respect to the attributes of quality. The new framework responds to emerging ICT trends in order to evolve government enterprise architecture continually and represent current architectural components and their relationships.}
}
@article{FUGINI2021100192,
  title    = {A Big Data Analytics Architecture for Smart Cities and Smart Companies},
  journal  = {Big Data Research},
  volume   = {24},
  pages    = {100192},
  year     = {2021},
  issn     = {2214-5796},
  doi      = {https://doi.org/10.1016/j.bdr.2021.100192},
  url      = {https://www.sciencedirect.com/science/article/pii/S2214579621000095},
  author   = {Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli},
  keywords = {Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities},
  abstract = {This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.}
}
@article{BALJAK2018275,
  title    = {A scalable realtime analytics pipeline and storage architecture for physiological monitoring big data},
  journal  = {Smart Health},
  volume   = {9-10},
  pages    = {275-286},
  year     = {2018},
  note     = {CHASE 2018 Special Issue},
  issn     = {2352-6483},
  doi      = {https://doi.org/10.1016/j.smhl.2018.07.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352648318300485},
  author   = {Valentina Baljak and Adis Ljubovic and Jonathan Michel and Mason Montgomery and Richard Salaway},
  keywords = {Data pipeline, IoHT, Big data, Physiological monitoring data, Real - time analytics, Healthcare},
  abstract = {Physiological monitors produce data essential to patient care. While the data is routinely observed and used at the bedside, it is rarely recorded permanently. Majority of data, especially high-resolution EKG waveforms, is discarded immediately. As the roles of big data and analytics in medicine are evolving, the streaming output of physiological monitors offers a potential source of highly relevant data for decision making and research. The inherently high volume and velocity of physiological data pose unique practical challenges for collection, storage, and analysis. A successful solution has to enable consistent and constant connectivity for streaming and to provide adequate storage and access capabilities. The solution also needs to meet security and privacy requirements for medical data. We propose a scalable, distributed architecture that leverages open-source stream processing software to connect raw monitoring output to a file storage system and an integration with existing data warehouse and data retrieval systems. Analytics pipeline has a potential to provide real-time feedback to the clinicians, including critical event detection or even prediction. The combination of real-time analysis and distributed storage of physiological big data, previously discarded, now opens up possibilities for future applications relevant to both clinicians and researchers alike. We have built and tested the first version of the continuously streaming data pipeline, from bedside physiological monitors to the storage. The results are promising, with raw or analyzed and enriched data already finding their place in supporting a variety of research use cases.}
}
@article{CHONG2017358,
  title    = {SeCBD: The Application Idea from Study Evaluation of Ransomware Attack Method in Big Data Architecture},
  journal  = {Procedia Computer Science},
  volume   = {116},
  pages    = {358-364},
  year     = {2017},
  note     = {Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2017.10.065},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050917321154},
  author   = {Henry Chong},
  keywords = {Ransomware, Crypto-Ransomware, Big Data, Application Idea, Evaluation Study, Offensive Encryption Attack},
  abstract = {Numerous ransomware attack was launched at May 2017 since it become emerge as trending for new cybercrime business source income model. The attack to several Big Data Architecture causing problem to over 150 countries. Meanwhile, the research on prevention idea of this attack method are still under struggle. This make ransomware becoming a serious and growing cyber issue. In spite of that, the objective of this paper is to share the application ideas from crypto-ransomware attack method toward future prevention mechanism application to make the Big Data Architecture more immune from future ransomware attack. The method used in this paper is systematic-review. The result is application prevention mechanism named as SeCBD which abbreviation for “Secure Big Data”. The application idea that result from the systematic review (SeCBD) concludes that most of crypto-ransomware still have chance to disabled, detected, or prevented.}
}
@article{PERSICO201898,
  title    = {Benchmarking big data architectures for social networks data processing using public cloud platforms},
  journal  = {Future Generation Computer Systems},
  volume   = {89},
  pages    = {98-109},
  year     = {2018},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2018.05.068},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17328303},
  author   = {Valerio Persico and Antonio Pescapé and Antonio Picariello and Giancarlo Sperlí},
  keywords = {On-line social networks, Lambda and Kappa architectures, Influence analysis, Social networking benchmarking, Public cloud benchmarking},
  abstract = {When considering popular On-line Social Networks (OSN) containing heterogeneous multimedia data sources, the complexity of the underlying processing systems becomes challenging, and requires to implement application-specific but still comprehensive benchmarking. The variety of big data architectures (and of their possible realization) for both batch and streaming processing in a huge number of application domains, makes the benchmarking of these systems critical for both academic and industrial communities. In this work, we evaluate the performance of two state-of-art big data architectures, namely Lambda and Kappa, considering OSN data analysis as reference task. In more details, we have implemented and deployed an influence analysis algorithm on the Microsoft Azure public cloud platform to investigate the impact of a number of factors on the performance obtained by cloud users. These factors comprise the type of the implemented architecture, the volume of the data to analyze, the size of the cluster of nodes realizing the architectures and their characteristics, the deployment costs, as well as the quality of the output when the analysis is subjected to strict temporal deadlines. Experimental campaigns have been carried out on the Yahoo Flickr Creative Commons 100 Million (YFCC100M). Reported results and discussions show that Lambda outperforms Kappa architecture for the class of problems investigated. Providing a variety of analyses – e.g., also investigating the impact of dataset size, scaling, cost – this paper provides useful insights on the performance of these state-of-art big data architectures that are helpful to both experts and newcomers interested in deploying big data architectures leveraging cloud platforms.}
}
@article{HUSSAIN2019100125,
  title   = {Corrigendum to “A Dynamic Neural Network Architecture with Immunology Inspired Optimization for Weather Data Forecasting” [Big Data Research 14 (2018) 81–92]},
  journal = {Big Data Research},
  volume  = {18},
  pages   = {100125},
  year    = {2019},
  issn    = {2214-5796},
  doi     = {https://doi.org/10.1016/j.bdr.2019.100125},
  url     = {https://www.sciencedirect.com/science/article/pii/S2214579619302278},
  author  = {Abir Jaafar Hussain and Panos Liatsis and Mohammed Khalaf and Hissam Tawfik and Haya Al-Asker}
}
@article{ZHANG2017626,
  title    = {A big data analytics architecture for cleaner manufacturing and maintenance processes of complex products},
  journal  = {Journal of Cleaner Production},
  volume   = {142},
  pages    = {626-641},
  year     = {2017},
  note     = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
  issn     = {0959-6526},
  doi      = {https://doi.org/10.1016/j.jclepro.2016.07.123},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652616310198},
  author   = {Yingfeng Zhang and Shan Ren and Yang Liu and Shubin Si},
  keywords = {Cleaner production, Product lifecycle, Manufacturing, Maintenance, Big data analytics, Data mining, Sustainable production},
  abstract = {Cleaner production (CP) is considered as one of the most important means for manufacturing enterprises to achieve sustainable production and improve their sustainable competitive advantage. However, implementation of the CP strategy was facing barriers, such as the lack of complete data and valuable knowledge that can be employed to provide better support on decision-making of coordination and optimization on the product lifecycle management (PLM) and the whole CP process. Fortunately, with the wide use of smart sensing devices in PLM, a large amount of real-time and multi-source lifecycle big data can now be collected. To make better PLM and CP decisions based on these data, in this paper, an overall architecture of big data-based analytics for product lifecycle (BDA-PL) was proposed. It integrated big data analytics and service-driven patterns that helped to overcome the above-mentioned barriers. Under the architecture, the availability and accessibility of data and knowledge related to the product were achieved. Focusing on manufacturing and maintenance process of the product lifecycle, and the key technologies were developed to implement the big data analytics. The presented architecture was demonstrated by an application scenario, and some observations and findings were discussed in details. The results showed that the proposed architecture benefited customers, manufacturers, environment and even all stages of PLM, and effectively promoted the implementation of CP. In addition, the managerial implications of the proposed architecture for four departments were analyzed and discussed. The new CP strategy provided a theoretical and practical basis for the sustainable development of manufacturing enterprises.}
}
@article{PETRILLO2020103177,
  title    = {Model-based vehicular prognostics framework using Big Data architecture},
  journal  = {Computers in Industry},
  volume   = {115},
  pages    = {103177},
  year     = {2020},
  issn     = {0166-3615},
  doi      = {https://doi.org/10.1016/j.compind.2019.103177},
  url      = {https://www.sciencedirect.com/science/article/pii/S0166361519304439},
  author   = {Alberto Petrillo and Antonio Picariello and Stefania Santini and Biagio Scarciello and Giancarlo Sperlí},
  keywords = {Model-based prognostic analysis, Big Data analysis, Cloud computing services},
  abstract = {Nowadays, the continuous technological advances allow designing novel Integrated Vehicle Health Management (IVHM) systems to deal with strict safety regulations in the automotive field with the aim at improving efficiency and reliability of automotive components. However, challenging issue, which arises in this domain, is handling a huge amount of data that are useful for prognostic. To this aim, in this paper we propose a cloud-based infrastructure, namely Automotive predicTOr Maintenance In Cloud (ATOMIC), for prognostic analysis that leverages Big Data technologies and mathematical models of both nominal and faulty behaviour of the automotive components to estimate on-line the End-Of-Life (EOL) and Remaining Useful Life (EUL) indicators for the automotive systems under investigation. A case study based on the Delphi DFG1596 fuel pump has been presented to evaluate the proposed prognostic method. Finally, we perform a benchmark analysis of the deployment configurations of ATOMIC architecture in terms of scalability and cost.}
}
@article{ROUKH202078,
  title    = {Big Data Processing Architecture for Smart Farming},
  journal  = {Procedia Computer Science},
  volume   = {177},
  pages    = {78-85},
  year     = {2020},
  note     = {The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2020) / The 10th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2020) / Affiliated Workshops},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2020.10.014},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050920322791},
  author   = {Amine Roukh and Fabrice Nolack Fote and Sidi Ahmed Mahmoudi and Saïd Mahmoudi},
  keywords = {Big Data, Smart Farming, Cloud platform},
  abstract = {In the era of Big data, data-driven farming is changing the agricultural businesses thanks to the use of modern technologies such as the Internet of Things (IoT) sensors, drones, and farm monitoring. IoT devices produce a massive amount of precious agri-data, which are collected and analyzed in real-time using innovative application tools. This combination of technology, known as “Smart farming”, helps various stakeholders in the agri-ecosystem to monitor crops in real-time, as well as maximize productivity and profitability in farm and business operations with the minimum efforts. Although many Smart farming solutions have been introduced, both from industry and academia, universal applicability of these approaches for other farms, unfortunately, is not feasible. Most of these solutions are based on a home-made non-standard Big data processing architecture. In this paper, we propose WALLeSMART, a cloud-based Smart farming management system, applied to the Wallonia region of Belgium. The framework introduces a general architecture to address the challenges of acquisition, processing, storing, and visualization of very large amounts of data, both in batch and real-time basis. An initial prototype has been developed and tested with various farms showing prominent results.}
}
@article{LI2019129,
  title    = {PIM-WEAVER: A High Energy-efficient, General-purpose Acceleration Architecture for String Operations in Big Data Processing},
  journal  = {Sustainable Computing: Informatics and Systems},
  volume   = {21},
  pages    = {129-142},
  year     = {2019},
  issn     = {2210-5379},
  doi      = {https://doi.org/10.1016/j.suscom.2019.01.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210537918301641},
  author   = {Wenming Li and Xiaochun Ye and Da Wang and Hao Zhang and Zhimin Tang and Dongrui Fan and Ninghui Sun},
  keywords = {PIM, String operations, Acceleration architecture, Big data, HMC},
  abstract = {The ever-growing data volume in big data era drives designers to find more efficient processor architectures both in performance and energy consumption. Among various computing patterns in big data applications, string operations are common but important parts of data processing. However, due to the consideration of generality, current general-purpose CPUs are not efficient in both performance and energy consumption when processing simple and fixed computation patterns of discrete string operations. On the other hand, moving massive data from memory to computing units through NoC, Cache hierarchies, and other memory access data paths is time-consuming and especially energy consuming. Fortunately, emerging technologies, such as Hybrid Memory Cube (HMC), enable the processing-in-memory (PIM) functionality without transferring massive data to remote processing units. In this paper, we propose PIM-WEAVER, a high-efficiency novel acceleration architecture for string processing using PIM mechanism, which is the 3D integration technology that facilitates stacking logic and memory dies in a single package. The PIM-WEAVER is implemented using such technology by integrating string processing units into the real world HMC memory. In PIM-WEAVER, the general-purpose acceleration architecture for string operation is implemented within the memory cube, which can reduce the latency of data transfer and also save energy. We also propose a full-stack solution of programming interface and control mechanism of instruction level. Our comprehensive evaluations using typical string processing algorithms from big data applications show that the PIM-WEAVER gains an average speedup of 14.74x over high-performance Intel processor, and reduces the energy consumption by 82.1% on average, with tiny area overhead.}
}
@article{PEI201817,
  title    = {Double-layered big data analytics architecture for solar cells series welding machine},
  journal  = {Computers in Industry},
  volume   = {97},
  pages    = {17-23},
  year     = {2018},
  issn     = {0166-3615},
  doi      = {https://doi.org/10.1016/j.compind.2018.01.019},
  url      = {https://www.sciencedirect.com/science/article/pii/S0166361517303299},
  author   = {Feng-Que Pei and Dong-Bo Li and Yi-Fei Tong},
  keywords = {Big data analytics, Solar cells series welding, MapReduce, Quality evaluation, Real-Time monitoring, Batch process},
  abstract = {The rapid and extensive pervasion of big data has enhanced the revolution of the society. A great interest has arisen in the past five years for mining and sharing the massive data. However, implementation of the big data analysis is facing many challenges, such as the storage, transmission, and computing. How to make the decision more intelligent in latency time becomes a crucial requirement for many researches. In this paper, a double-layered architecture ATWDP of online and offline analytics for solar cells series welding machine industry is proposed and the distributed and parallel computing system can handle the above challenges. The ATWDP offers an approach to analyze the data gathered from various sensing devices stably and efficiently. Some key implementation technologies in the system are discussed, especially the Hadoop Apache framework. The evaluation of service quality based on Support Vector Machine-Dempster Shafer Theory (SVMs-DS) and Spark is an application scenario to illustrate the mechanism of the ATWDP. And a data set is used to verify the rationality of the ATWDP in the storage and processing. Test results show that the ATWDP platform has a good performance and is a suitable solution for dealing with the big data of solar cells series welding machine.}
}
@article{MAYO2016E417,
  title   = {Taming Big Data: Implementation of a Clinical Use-Case Driven Architecture},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  volume  = {96},
  number  = {2, Supplement },
  pages   = {E417-E418},
  year    = {2016},
  note    = {Proceedings of the American Society for Radiation Oncology},
  issn    = {0360-3016},
  doi     = {https://doi.org/10.1016/j.ijrobp.2016.06.1680},
  url     = {https://www.sciencedirect.com/science/article/pii/S0360301616320065},
  author  = {C. Mayo and M.L. Kessler and M. Feng and G. Weyburn and I. {El Naqa} and A. Eisbruch and M.M. Matuszak and D. McShan and J.M. Moran and C.J. Anderson and L. Holevinski and D. Owen and R.K. Ten Haken}
}
@article{MANOGARAN2018375,
  title    = {A new architecture of Internet of Things and big data ecosystem for secured smart healthcare monitoring and alerting system},
  journal  = {Future Generation Computer Systems},
  volume   = {82},
  pages    = {375-387},
  year     = {2018},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2017.10.045},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17305149},
  author   = {Gunasekaran Manogaran and R. Varatharajan and Daphne Lopez and Priyan Malarvizhi Kumar and Revathi Sundarasekar and Chandu Thota},
  keywords = {Wireless sensor networks, Internet of Things, Big data analytics, Cloud computing and health car},
  abstract = {Wearable medical devices with sensor continuously generate enormous data which is often called as big data mixed with structured and unstructured data. Due to the complexity of the data, it is difficult to process and analyze the big data for finding valuable information that can be useful in decision-making. On the other hand, data security is a key requirement in healthcare big data system. In order to overcome this issue, this paper proposes a new architecture for the implementation of IoT to store and process scalable sensor data (big data) for health care applications. The Proposed architecture consists of two main sub architectures, namely, Meta Fog-Redirection (MF-R) and Grouping and Choosing (GC) architecture. MF-R architecture uses big data technologies such as Apache Pig and Apache HBase for collection and storage of the sensor data (big data) generated from different sensor devices. The proposed GC architecture is used for securing integration of fog computing with cloud computing. This architecture also uses key management service and data categorization function (Sensitive, Critical and Normal) for providing security services. The framework also uses MapReduce based prediction model to predict the heart diseases. Performance evaluation parameters such as throughput, sensitivity, accuracy, and f-measure are calculated to prove the efficiency of the proposed architecture as well as the prediction model.}
}
@article{CASTELLANOS2021110869,
  title    = {ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures},
  journal  = {Journal of Systems and Software},
  volume   = {172},
  pages    = {110869},
  year     = {2021},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/j.jss.2020.110869},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121220302594},
  author   = {Camilo Castellanos and Carlos A. Varela and Dario Correal},
  keywords = {Software architecture, Big data analytics deployment, DevOps, Domain-specific model, Quality scenarios, Performance monitoring},
  abstract = {Big data analytics (BDA) applications use machine learning algorithms to extract valuable insights from large, fast, and heterogeneous data sources. New software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments, longer development cycles, and challenging performance assessment. This paper proposes a Domain-Specific Model (DSM), and DevOps practices to design, deploy, and monitor performance metrics in BDA applications. Our proposal includes a design process, and a framework to define architectural inputs, software components, and deployment strategies through integrated high-level abstractions to enable QS monitoring. We evaluate our approach with four use cases from different domains to demonstrate a high level of generalization. Our results show a shorter deployment and monitoring times, and a higher gain factor per iteration compared to similar approaches.}
}
@article{CAMPOS2017369,
  title    = {A Big Data Analytical Architecture for the Asset Management},
  journal  = {Procedia CIRP},
  volume   = {64},
  pages    = {369-374},
  year     = {2017},
  note     = {9th CIRP IPSS Conference: Circular Perspectives on PSS},
  issn     = {2212-8271},
  doi      = {https://doi.org/10.1016/j.procir.2017.03.019},
  url      = {https://www.sciencedirect.com/science/article/pii/S2212827117301634},
  author   = {Jaime Campos and Pankaj Sharma and Unai Gorostegui Gabiria and Erkki Jantunen and David Baglee},
  keywords = {Asset Management, Big data, Big data analytics, Data mining},
  abstract = {The paper highlights the characteristics of data and big data analytics in manufacturing, more specifically for the industrial asset management. The authors highlight important aspects of the analytical system architecture for purposes of asset management. The authors cover the data and big data technology aspects of the domain of interest. This is followed by application of the big data analytics and technologies, such as machine learning and data mining for asset management. The paper also presents the aspects of visualisation of the results of data analytics. In conclusion, the architecture provides a holistic view of the aspects and requirements of a big data technology application system for purposes of asset management. The issues addressed in the paper, namely equipment health, reliability, effects of unplanned breakdown, etc., are extremely important for today's manufacturing companies. Moreover, the customer's opinion and preferences of the product/services are crucial as it gives an insight into the ways to improve in order to stay competitive in the market. Finally, a successful asset management function plays an important role in the manufacturing industry, which is dependent on the support of proper ICTs for its further success.}
}
@incollection{TEKINERDOGAN2017127,
  title     = {Chapter 8 - Performance Isolation in Cloud-Based Big Data Architectures},
  editor    = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
  booktitle = {Software Architecture for Big Data and the Cloud},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {127-145},
  year      = {2017},
  isbn      = {978-0-12-805467-3},
  doi       = {https://doi.org/10.1016/B978-0-12-805467-3.00008-9},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128054673000089},
  author    = {Bedir Tekinerdogan and Alp Oral},
  keywords  = {Software architecture modeling, Architectural views, Runtime configuration, Dynamic configuration, Runtime adaptability},
  abstract  = {Cloud-based big data systems usually have many different tenants that require access to the server's functionality. In a nonisolated cloud system, the different tenants can freely use the resources of the server. Hereby, disruptive tenants who exceed their limits can easily cause degradation of performance of the provided services for other tenants. To ensure performance demands of the multiple tenants and meet fairness criteria, various performance isolation approaches have been introduced including artificial delay, round robin, blacklist, and thread pool. Each of these performance isolation approaches adopts different strategies to avoid the performance interference in case of multiple concurrent tenant needs. In this paper, we propose a framework and a systematic approach for performance isolation in cloud-based big data systems. To this end, we present an architecture design of cloud-based big data system and discuss the integration of feasible performance isolation approaches. We evaluate our approach using PublicFeed, a social media application that is based on a cloud-based big data platform.}
}
@incollection{CHESSELL201733,
  title     = {Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture},
  editor    = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
  booktitle = {Software Architecture for Big Data and the Cloud},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {33-48},
  year      = {2017},
  isbn      = {978-0-12-805467-3},
  doi       = {https://doi.org/10.1016/B978-0-12-805467-3.00003-X},
  url       = {https://www.sciencedirect.com/science/article/pii/B978012805467300003X},
  author    = {Mandy Chessell and Dan Wolfson and Tim Vincent},
  keywords  = {Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains},
  abstract  = {Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.}
}
@article{LIU2021262,
  title    = {Hybrid Intelligence in Big Data Environment: Concepts, Architectures, and Applications of Intelligent Service},
  journal  = {Data and Information Management},
  volume   = {5},
  number   = {2},
  pages    = {262-276},
  year     = {2021},
  issn     = {2543-9251},
  doi      = {https://doi.org/10.2478/dim-2020-0051},
  url      = {https://www.sciencedirect.com/science/article/pii/S2543925122000134},
  author   = {Zhenghao Liu and Xi Zeng},
  keywords = {human–machine hybrid Intelligence, human–machine interaction, artificial intelligence, knowledge fusion, intelligence service},
  abstract = {Based on the emerging concept of “Hybrid Intelligence,” this paper aims to explore a new model of human–computer interaction, and deeply research on its development and application of Intelligent Service in the big data environment. It systematically explores the related academic concepts of hybrid intelligence, and establishes its architecture model. The development of hybrid intelligence is faced with cognitive differences, system fragmentation, human–machine digital divide, and other issues. Strengthening the interaction between cognition and perception can be the key to break through the bottleneck. The intelligent service system based on the hybrid intelligent architecture takes knowledge fusion as the core, and “cloud intelligent brain” is making it possible for the human–computer symbiosis driven by hybrid intelligence. The proposed advanced human–computer interaction mode constructs a hybrid intelligent architecture model, enriches the concept system of human–machine hybrid intelligence, and provides a new landing scheme for intelligent services based on complex scenes in the big data environment.}
}
@article{BRADLEY201758,
  title    = {Parallel Design of a Product and Internet of Things (IoT) Architecture to Minimize the Cost of Utilizing Big Data (BD) for Sustainable Value Creation},
  journal  = {Procedia CIRP},
  volume   = {61},
  pages    = {58-62},
  year     = {2017},
  note     = {The 24th CIRP Conference on Life Cycle Engineering},
  issn     = {2212-8271},
  doi      = {https://doi.org/10.1016/j.procir.2016.11.213},
  url      = {https://www.sciencedirect.com/science/article/pii/S2212827116313828},
  author   = {Ryan Bradley and I.S. Jawahir and Niko Murrell and Julie Whitney},
  keywords = {Internet of Things, Big Data, Product Design, Machine Learning, Sustainability},
  abstract = {Information has become today's addictive currency; hence, companies are investing billions in the creation of Internet of Things (IoT) frameworks that gamble on finding trends that reveal sustainability and/or efficiency improvements. This approach to “Big Data” can lead to blind, astronomical costs. Therefore, this paper presents a counter approach aimed at minimizing the cost of utilizing “Big Data” for sustainable value creation. The proposed approach leverages domain/expert knowledge of the system in combination with a machine learning algorithm in order to limit the needed infrastructure and cost. A case study of the approach implemented in a consumer electronics company is also included.}
}
@article{MOHAPATRA2016236,
  title    = {Big data analytic architecture for intruder detection in heterogeneous wireless sensor networks},
  journal  = {Journal of Network and Computer Applications},
  volume   = {66},
  pages    = {236-249},
  year     = {2016},
  issn     = {1084-8045},
  doi      = {https://doi.org/10.1016/j.jnca.2016.03.004},
  url      = {https://www.sciencedirect.com/science/article/pii/S108480451630011X},
  author   = {Suvendu Kumar Mohapatra and Prasan Kumar Sahoo and Shih-Lin Wu},
  keywords = {WSN, Barrier coverage, Intruder detection, Big data, Spark},
  abstract = {Barrier coverage in Wireless Sensor Networks (WSNs) is an important research issue as intruder detection is the main purpose of deploying wireless sensors over a specified monitoring region. In WSNs, excessive volume and variety of sensor data are generated, which need to be analyzed for accurate measurement of the image in terms of width and resolution. In this paper, a three layered big data analytic architecture is designed to analyze the data generated during the construction of the barrier and detection of the intruder using camera sensors. Besides, a cloud layer is designed for storing the analyzed data to study the behavior of the intruder. In order to minimize the number of camera sensors for constructing the barrier, algorithms are designed to construct the single barrier with limited node mobility and the barrier path Quality of Sensing (QoS) is maintained with a minimum number of camera sensors. Simulation results show that our algorithms can construct 100% of the barrier with fewer number of camera sensors and average data processing time can be reduced by using parallel servers even if for larger size of data.}
}
@article{NARAYANAN2020,
  title    = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  year     = {2020},
  issn     = {1319-1578},
  doi      = {https://doi.org/10.1016/j.jksuci.2020.05.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
  author   = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
  keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
  abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@article{GUO2017519,
  title    = {Object detection among multimedia big data in the compressive measurement domain under mobile distributed architecture},
  journal  = {Future Generation Computer Systems},
  volume   = {76},
  pages    = {519-527},
  year     = {2017},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2017.03.004},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X17303333},
  author   = {Jie Guo and Bin Song and Fei {Richard Yu} and Zheng Yan and Laurence T. Yang},
  keywords = {Exploration method, Multimedia big data, Compressed sensing, Distributed computing architecture},
  abstract = {Multimedia big data is difficult to handle because of its enormous amount and the elusive property of underlying information. To study how to explore valuable information among multimedia big data with low complexity, this paper proposes an object detection method of big data, which is in compressive measurement domain under a mobile distributed computing architecture. It includes the sparse representation and object detection processes. Considering the unbalanced computation capacity between a mobile center cloud and mobile edge sites, we shift large storage burden into the cloud, while performing the dictionary learning by using compressive measurements in the mobile edge sites. Specifically, after getting the measurements at the edge sites, we perform dictionary learning to obtain the sparse representation in pixel domain, then select significant images and their feature vectors to be stored in the center cloud. In addition, we also analyze the trained dictionary in the measurement domain employing measurements. In order to reveal the two kinds of dictionaries’ relationship, we conduct a formulation process into each of them and find that the relationship depends on the uniqueness relation between the original signal and the sparse coefficient in the measurement domain. At the same time, we keep coefficients for a certain time period at the mobile edge sites in order to realize real time object detection, taking the advantage of low latency of the mobile edge computing ends. Since the sparse coefficients and the original signal have a one-to-one correspondence relationship, we can just search for the matched coefficients of the image block for detecting object. Experimental results show that Hadamard measurement matrix can better preserve the characteristics of the original signal than Gaussian matrix and that the proposed method can achieve a favorable detection performance. Meanwhile, the computation cost and storage cost of the proposed detection process can be significantly reduced compared with traditional methods, which is suitable for the multimedia big data. This can also be used in smart cities for looking for lost children and other specific events.}
}
@article{ULLAH201981,
  title    = {Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review},
  journal  = {Journal of Systems and Software},
  volume   = {151},
  pages    = {81-118},
  year     = {2019},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/j.jss.2019.01.051},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121219300172},
  author   = {Faheem Ullah and Muhammad {Ali Babar}},
  keywords = {Big data, Cybersecurity, Quality attribute, Architectural tactic},
  abstract = {Context
              Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
              Objective
              We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
              Method
              We used Systematic Literature Review (SLR) method for reviewing 74 papers.
              Result
              Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
              Conclusion
              Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}
@article{DEV20191076,
  title    = {Multi-criteria evaluation of real-time key performance indicators of supply chain with consideration of big data architecture},
  journal  = {Computers & Industrial Engineering},
  volume   = {128},
  pages    = {1076-1087},
  year     = {2019},
  issn     = {0360-8352},
  doi      = {https://doi.org/10.1016/j.cie.2018.04.012},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360835218301487},
  author   = {Navin K. Dev and Ravi Shankar and Rachita Gupta and Jingxin Dong},
  keywords = {Big data, Real-time mechanism, Key performance indicators, Fuzzy-ANP, Discrete event simulation, Supply chain management},
  abstract = {One of the major issues a designer of Big Data Architecture has to trade with is incorporating real-time predictive analytics capability using offline synergistic approaches like simulation, fuzzy analytic network process, and Technique for Order Preference. Further, under this setting, which involves re-engineering of operational units, the present study proposes a simple, yet practical heuristic to quickly handle the unstructured relational key-performance-indicators (KPIs) data of a supply chain that are obtained from the results of the simulation. Within the big data framework, the proposed model can be used as a decision support tool by the companies to evaluate their KPIs in a real-time dynamic system.}
}